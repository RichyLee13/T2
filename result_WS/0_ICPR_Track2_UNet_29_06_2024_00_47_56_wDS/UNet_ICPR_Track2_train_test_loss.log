epoch: 0.000000:	train_loss: 0.999679:	test_loss: 2.123468:	
epoch: 1.000000:	train_loss: 1.602659:	test_loss: 1.340602:	
epoch: 2.000000:	train_loss: 1.368407:	test_loss: 1.341985:	
epoch: 3.000000:	train_loss: 1.347647:	test_loss: 1.339977:	
epoch: 4.000000:	train_loss: 1.349786:	test_loss: 1.339923:	
epoch: 5.000000:	train_loss: 1.343544:	test_loss: 1.344837:	
epoch: 6.000000:	train_loss: 1.342435:	test_loss: 1.340007:	
epoch: 7.000000:	train_loss: 1.341648:	test_loss: 1.342480:	
epoch: 8.000000:	train_loss: 1.336802:	test_loss: 1.340891:	
epoch: 9.000000:	train_loss: 1.335348:	test_loss: 1.340484:	
epoch: 10.000000:	train_loss: 1.337189:	test_loss: 1.342152:	
epoch: 11.000000:	train_loss: 1.337612:	test_loss: 1.341549:	
epoch: 12.000000:	train_loss: 1.333526:	test_loss: 1.342388:	
epoch: 13.000000:	train_loss: 1.332519:	test_loss: 1.342260:	
epoch: 14.000000:	train_loss: 1.340884:	test_loss: 1.340685:	
epoch: 15.000000:	train_loss: 1.334524:	test_loss: 1.344072:	
epoch: 16.000000:	train_loss: 1.339785:	test_loss: 1.344354:	
epoch: 17.000000:	train_loss: 1.337011:	test_loss: 1.343183:	
epoch: 18.000000:	train_loss: 1.335820:	test_loss: 1.343857:	
epoch: 19.000000:	train_loss: 1.339400:	test_loss: 1.342743:	
epoch: 20.000000:	train_loss: 1.330621:	test_loss: 1.340945:	
epoch: 21.000000:	train_loss: 1.340008:	test_loss: 1.342190:	
epoch: 22.000000:	train_loss: 1.334238:	test_loss: 1.343738:	
epoch: 23.000000:	train_loss: 1.331747:	test_loss: 1.341070:	
epoch: 24.000000:	train_loss: 1.331234:	test_loss: 1.343914:	
epoch: 25.000000:	train_loss: 1.330094:	test_loss: 1.345085:	
epoch: 26.000000:	train_loss: 1.330533:	test_loss: 1.340449:	
epoch: 27.000000:	train_loss: 1.328146:	test_loss: 1.337756:	
epoch: 28.000000:	train_loss: 1.328269:	test_loss: 1.337987:	
epoch: 29.000000:	train_loss: 1.330521:	test_loss: 1.345032:	
epoch: 30.000000:	train_loss: 1.330732:	test_loss: 1.343893:	
epoch: 31.000000:	train_loss: 1.328644:	test_loss: 1.338367:	
epoch: 32.000000:	train_loss: 1.331391:	test_loss: 1.335568:	
epoch: 33.000000:	train_loss: 1.326790:	test_loss: 1.337100:	
epoch: 34.000000:	train_loss: 1.327113:	test_loss: 1.335868:	
epoch: 35.000000:	train_loss: 1.327662:	test_loss: 1.344712:	
epoch: 36.000000:	train_loss: 1.326639:	test_loss: 1.334370:	
epoch: 37.000000:	train_loss: 1.336360:	test_loss: 1.340184:	
epoch: 38.000000:	train_loss: 1.329273:	test_loss: 1.338684:	
epoch: 39.000000:	train_loss: 1.326764:	test_loss: 1.344505:	
epoch: 40.000000:	train_loss: 1.328063:	test_loss: 1.338719:	
epoch: 41.000000:	train_loss: 1.330461:	test_loss: 1.339621:	
epoch: 42.000000:	train_loss: 1.328829:	test_loss: 1.335501:	
epoch: 43.000000:	train_loss: 1.323247:	test_loss: 1.340204:	
epoch: 44.000000:	train_loss: 1.328090:	test_loss: 1.342288:	
epoch: 45.000000:	train_loss: 1.327226:	test_loss: 1.336606:	
epoch: 46.000000:	train_loss: 1.328068:	test_loss: 1.332632:	
epoch: 47.000000:	train_loss: 1.324508:	test_loss: 1.339094:	
epoch: 48.000000:	train_loss: 1.328988:	test_loss: 1.335914:	
epoch: 49.000000:	train_loss: 1.324222:	test_loss: 1.332597:	
epoch: 50.000000:	train_loss: 1.325516:	test_loss: 1.331883:	
epoch: 51.000000:	train_loss: 1.320273:	test_loss: 1.336277:	
epoch: 52.000000:	train_loss: 1.325796:	test_loss: 1.331954:	
epoch: 53.000000:	train_loss: 1.323332:	test_loss: 1.341129:	
epoch: 54.000000:	train_loss: 1.321382:	test_loss: 1.331286:	
epoch: 55.000000:	train_loss: 1.328493:	test_loss: 1.333777:	
epoch: 56.000000:	train_loss: 1.326402:	test_loss: 1.334422:	
epoch: 57.000000:	train_loss: 1.322896:	test_loss: 1.330605:	
epoch: 58.000000:	train_loss: 1.324874:	test_loss: 1.342788:	
epoch: 59.000000:	train_loss: 1.323607:	test_loss: 1.334692:	
epoch: 60.000000:	train_loss: 1.322920:	test_loss: 1.333704:	
epoch: 61.000000:	train_loss: 1.322990:	test_loss: 1.334503:	
epoch: 62.000000:	train_loss: 1.324556:	test_loss: 1.340736:	
epoch: 63.000000:	train_loss: 1.324858:	test_loss: 1.346569:	
epoch: 64.000000:	train_loss: 1.322103:	test_loss: 1.335993:	
epoch: 65.000000:	train_loss: 1.323565:	test_loss: 1.329943:	
epoch: 66.000000:	train_loss: 1.320633:	test_loss: 1.335323:	
epoch: 67.000000:	train_loss: 1.324453:	test_loss: 1.340396:	
epoch: 68.000000:	train_loss: 1.322113:	test_loss: 1.335101:	
epoch: 69.000000:	train_loss: 1.328030:	test_loss: 1.329720:	
epoch: 70.000000:	train_loss: 1.326610:	test_loss: 1.346733:	
epoch: 71.000000:	train_loss: 1.324000:	test_loss: 1.339346:	
epoch: 72.000000:	train_loss: 1.323136:	test_loss: 1.338137:	
epoch: 73.000000:	train_loss: 1.322340:	test_loss: 1.332887:	
epoch: 74.000000:	train_loss: 1.320718:	test_loss: 1.337288:	
epoch: 75.000000:	train_loss: 1.320323:	test_loss: 1.336268:	
epoch: 76.000000:	train_loss: 1.328265:	test_loss: 1.338181:	
epoch: 77.000000:	train_loss: 1.326624:	test_loss: 1.333791:	
epoch: 78.000000:	train_loss: 1.321860:	test_loss: 1.357157:	
epoch: 79.000000:	train_loss: 1.323836:	test_loss: 1.333812:	
epoch: 80.000000:	train_loss: 1.325613:	test_loss: 1.330445:	
epoch: 81.000000:	train_loss: 1.317026:	test_loss: 1.331178:	
epoch: 82.000000:	train_loss: 1.321574:	test_loss: 1.339840:	
epoch: 83.000000:	train_loss: 1.321895:	test_loss: 1.335072:	
epoch: 84.000000:	train_loss: 1.317079:	test_loss: 1.350402:	
epoch: 85.000000:	train_loss: 1.320507:	test_loss: 1.336426:	
epoch: 86.000000:	train_loss: 1.322420:	test_loss: 1.333750:	
epoch: 87.000000:	train_loss: 1.321147:	test_loss: 1.333188:	
epoch: 88.000000:	train_loss: 1.322809:	test_loss: 1.344931:	
epoch: 89.000000:	train_loss: 1.320736:	test_loss: 1.330151:	
epoch: 90.000000:	train_loss: 1.325051:	test_loss: 1.338226:	
epoch: 91.000000:	train_loss: 1.322338:	test_loss: 1.332875:	
epoch: 92.000000:	train_loss: 1.322593:	test_loss: 1.335396:	
epoch: 93.000000:	train_loss: 1.318200:	test_loss: 1.331011:	
epoch: 94.000000:	train_loss: 1.317831:	test_loss: 1.343982:	
epoch: 95.000000:	train_loss: 1.318663:	test_loss: 1.343425:	
epoch: 96.000000:	train_loss: 1.324260:	test_loss: 1.337831:	
epoch: 97.000000:	train_loss: 1.317491:	test_loss: 1.338586:	
epoch: 98.000000:	train_loss: 1.323705:	test_loss: 1.340588:	
epoch: 99.000000:	train_loss: 1.322950:	test_loss: 1.336327:	
epoch: 100.000000:	train_loss: 1.321977:	test_loss: 1.331347:	
epoch: 101.000000:	train_loss: 1.322800:	test_loss: 1.339448:	
epoch: 102.000000:	train_loss: 1.318110:	test_loss: 1.336696:	
epoch: 103.000000:	train_loss: 1.320682:	test_loss: 1.347418:	
epoch: 104.000000:	train_loss: 1.320219:	test_loss: 1.328332:	
epoch: 105.000000:	train_loss: 1.322759:	test_loss: 1.338252:	
epoch: 106.000000:	train_loss: 1.323165:	test_loss: 1.337566:	
epoch: 107.000000:	train_loss: 1.321257:	test_loss: 1.330590:	
epoch: 108.000000:	train_loss: 1.319535:	test_loss: 1.335566:	
epoch: 109.000000:	train_loss: 1.323155:	test_loss: 1.330177:	
epoch: 110.000000:	train_loss: 1.320575:	test_loss: 1.341077:	
epoch: 111.000000:	train_loss: 1.321611:	test_loss: 1.334353:	
epoch: 112.000000:	train_loss: 1.323669:	test_loss: 1.330443:	
epoch: 113.000000:	train_loss: 1.321510:	test_loss: 1.329167:	
epoch: 114.000000:	train_loss: 1.314874:	test_loss: 1.343396:	
epoch: 115.000000:	train_loss: 1.314894:	test_loss: 1.337843:	
epoch: 116.000000:	train_loss: 1.320658:	test_loss: 1.333361:	
epoch: 117.000000:	train_loss: 1.318237:	test_loss: 1.340141:	
epoch: 118.000000:	train_loss: 1.319121:	test_loss: 1.332333:	
epoch: 119.000000:	train_loss: 1.321982:	test_loss: 1.344692:	
epoch: 120.000000:	train_loss: 1.319220:	test_loss: 1.329614:	
epoch: 121.000000:	train_loss: 1.317659:	test_loss: 1.333010:	
epoch: 122.000000:	train_loss: 1.318388:	test_loss: 1.333520:	
epoch: 123.000000:	train_loss: 1.318898:	test_loss: 1.339858:	
epoch: 124.000000:	train_loss: 1.320265:	test_loss: 1.337805:	
epoch: 125.000000:	train_loss: 1.313845:	test_loss: 1.343182:	
epoch: 126.000000:	train_loss: 1.321055:	test_loss: 1.329942:	
epoch: 127.000000:	train_loss: 1.319576:	test_loss: 1.335028:	
epoch: 128.000000:	train_loss: 1.319706:	test_loss: 1.332958:	
epoch: 129.000000:	train_loss: 1.318199:	test_loss: 1.339606:	
epoch: 130.000000:	train_loss: 1.323090:	test_loss: 1.334398:	
epoch: 131.000000:	train_loss: 1.320114:	test_loss: 1.337703:	
epoch: 132.000000:	train_loss: 1.323984:	test_loss: 1.328836:	
epoch: 133.000000:	train_loss: 1.317138:	test_loss: 1.333642:	
epoch: 134.000000:	train_loss: 1.320540:	test_loss: 1.341469:	
epoch: 135.000000:	train_loss: 1.319881:	test_loss: 1.330147:	
epoch: 136.000000:	train_loss: 1.313399:	test_loss: 1.331734:	
epoch: 137.000000:	train_loss: 1.319040:	test_loss: 1.337291:	
epoch: 138.000000:	train_loss: 1.316663:	test_loss: 1.337883:	
epoch: 139.000000:	train_loss: 1.315698:	test_loss: 1.333412:	
epoch: 140.000000:	train_loss: 1.320929:	test_loss: 1.331201:	
epoch: 141.000000:	train_loss: 1.315228:	test_loss: 1.331916:	
epoch: 142.000000:	train_loss: 1.298713:	test_loss: 1.261508:	
epoch: 143.000000:	train_loss: 1.266672:	test_loss: 1.283052:	
epoch: 144.000000:	train_loss: 1.256895:	test_loss: 1.205752:	
epoch: 145.000000:	train_loss: 1.244230:	test_loss: 1.165980:	
epoch: 146.000000:	train_loss: 1.243749:	test_loss: 1.173883:	
epoch: 147.000000:	train_loss: 1.240943:	test_loss: 1.168360:	
epoch: 148.000000:	train_loss: 1.233461:	test_loss: 1.156481:	
epoch: 149.000000:	train_loss: 1.225777:	test_loss: 1.170352:	
epoch: 150.000000:	train_loss: 1.224210:	test_loss: 1.143673:	
epoch: 151.000000:	train_loss: 1.221128:	test_loss: 1.163434:	
epoch: 152.000000:	train_loss: 1.216566:	test_loss: 1.153515:	
epoch: 153.000000:	train_loss: 1.213812:	test_loss: 1.134408:	
epoch: 154.000000:	train_loss: 1.203212:	test_loss: 1.129812:	
epoch: 155.000000:	train_loss: 1.196488:	test_loss: 1.160790:	
epoch: 156.000000:	train_loss: 1.196232:	test_loss: 1.112215:	
epoch: 157.000000:	train_loss: 1.189189:	test_loss: 1.102119:	
epoch: 158.000000:	train_loss: 1.188838:	test_loss: 1.080350:	
epoch: 159.000000:	train_loss: 1.186067:	test_loss: 1.080699:	
epoch: 160.000000:	train_loss: 1.179928:	test_loss: 1.086297:	
epoch: 161.000000:	train_loss: 1.178284:	test_loss: 1.035702:	
epoch: 162.000000:	train_loss: 1.172182:	test_loss: 1.056124:	
epoch: 163.000000:	train_loss: 1.166096:	test_loss: 1.054537:	
epoch: 164.000000:	train_loss: 1.166522:	test_loss: 1.039407:	
epoch: 165.000000:	train_loss: 1.166309:	test_loss: 1.049803:	
epoch: 166.000000:	train_loss: 1.160912:	test_loss: 1.102289:	
epoch: 167.000000:	train_loss: 1.154528:	test_loss: 1.056585:	
epoch: 168.000000:	train_loss: 1.157407:	test_loss: 1.069935:	
epoch: 169.000000:	train_loss: 1.155673:	test_loss: 1.025237:	
epoch: 170.000000:	train_loss: 1.154549:	test_loss: 1.061149:	
epoch: 171.000000:	train_loss: 1.150642:	test_loss: 1.022714:	
epoch: 172.000000:	train_loss: 1.152135:	test_loss: 1.061965:	
epoch: 173.000000:	train_loss: 1.151468:	test_loss: 1.052552:	
epoch: 174.000000:	train_loss: 1.141124:	test_loss: 1.071864:	
epoch: 175.000000:	train_loss: 1.145910:	test_loss: 1.034003:	
epoch: 176.000000:	train_loss: 1.149273:	test_loss: 1.019062:	
epoch: 177.000000:	train_loss: 1.143994:	test_loss: 1.028811:	
epoch: 178.000000:	train_loss: 1.142791:	test_loss: 1.083543:	
epoch: 179.000000:	train_loss: 1.140693:	test_loss: 1.018962:	
epoch: 180.000000:	train_loss: 1.138319:	test_loss: 1.035288:	
epoch: 181.000000:	train_loss: 1.135611:	test_loss: 1.027966:	
epoch: 182.000000:	train_loss: 1.136586:	test_loss: 1.003288:	
epoch: 183.000000:	train_loss: 1.144069:	test_loss: 1.014315:	
epoch: 184.000000:	train_loss: 1.138701:	test_loss: 0.991701:	
epoch: 185.000000:	train_loss: 1.133712:	test_loss: 0.999731:	
epoch: 186.000000:	train_loss: 1.134157:	test_loss: 1.003026:	
epoch: 187.000000:	train_loss: 1.130945:	test_loss: 1.014507:	
epoch: 188.000000:	train_loss: 1.128125:	test_loss: 1.023671:	
epoch: 189.000000:	train_loss: 1.134646:	test_loss: 1.008219:	
epoch: 190.000000:	train_loss: 1.132805:	test_loss: 0.998697:	
epoch: 191.000000:	train_loss: 1.126197:	test_loss: 1.023313:	
epoch: 192.000000:	train_loss: 1.125832:	test_loss: 1.006052:	
epoch: 193.000000:	train_loss: 1.128653:	test_loss: 1.000704:	
epoch: 194.000000:	train_loss: 1.125565:	test_loss: 0.998194:	
epoch: 195.000000:	train_loss: 1.121230:	test_loss: 0.995113:	
epoch: 196.000000:	train_loss: 1.128804:	test_loss: 0.991278:	
epoch: 197.000000:	train_loss: 1.126157:	test_loss: 1.013669:	
epoch: 198.000000:	train_loss: 1.128957:	test_loss: 0.988196:	
epoch: 199.000000:	train_loss: 1.124495:	test_loss: 0.975233:	
epoch: 200.000000:	train_loss: 1.120934:	test_loss: 0.997798:	
epoch: 201.000000:	train_loss: 1.124155:	test_loss: 1.006451:	
epoch: 202.000000:	train_loss: 1.118849:	test_loss: 0.997305:	
epoch: 203.000000:	train_loss: 1.107398:	test_loss: 0.973840:	
epoch: 204.000000:	train_loss: 1.102602:	test_loss: 0.985305:	
epoch: 205.000000:	train_loss: 1.102501:	test_loss: 1.010222:	
epoch: 206.000000:	train_loss: 1.096931:	test_loss: 0.988612:	
epoch: 207.000000:	train_loss: 1.096386:	test_loss: 0.983179:	
epoch: 208.000000:	train_loss: 1.084743:	test_loss: 0.997883:	
epoch: 209.000000:	train_loss: 1.084794:	test_loss: 1.022578:	
epoch: 210.000000:	train_loss: 1.076604:	test_loss: 1.005586:	
epoch: 211.000000:	train_loss: 1.081416:	test_loss: 0.972037:	
epoch: 212.000000:	train_loss: 1.080520:	test_loss: 1.005465:	
epoch: 213.000000:	train_loss: 1.073699:	test_loss: 0.989166:	
epoch: 214.000000:	train_loss: 1.069656:	test_loss: 0.980206:	
epoch: 215.000000:	train_loss: 1.071257:	test_loss: 0.976338:	
epoch: 216.000000:	train_loss: 1.071444:	test_loss: 0.970504:	
epoch: 217.000000:	train_loss: 1.066869:	test_loss: 0.983987:	
epoch: 218.000000:	train_loss: 1.065019:	test_loss: 0.980716:	
epoch: 219.000000:	train_loss: 1.058649:	test_loss: 0.979142:	
epoch: 220.000000:	train_loss: 1.060880:	test_loss: 1.011302:	
epoch: 221.000000:	train_loss: 1.059742:	test_loss: 0.972259:	
epoch: 222.000000:	train_loss: 1.058949:	test_loss: 0.978958:	
epoch: 223.000000:	train_loss: 1.063511:	test_loss: 0.981811:	
epoch: 224.000000:	train_loss: 1.060621:	test_loss: 0.978574:	
epoch: 225.000000:	train_loss: 1.070392:	test_loss: 1.008144:	
epoch: 226.000000:	train_loss: 1.058714:	test_loss: 0.980433:	
epoch: 227.000000:	train_loss: 1.058220:	test_loss: 1.045587:	
epoch: 228.000000:	train_loss: 1.058342:	test_loss: 1.003857:	
epoch: 229.000000:	train_loss: 1.052588:	test_loss: 0.985021:	
epoch: 230.000000:	train_loss: 1.057448:	test_loss: 0.966387:	
epoch: 231.000000:	train_loss: 1.049126:	test_loss: 0.994439:	
epoch: 232.000000:	train_loss: 1.047363:	test_loss: 0.972554:	
epoch: 233.000000:	train_loss: 1.051489:	test_loss: 0.989523:	
epoch: 234.000000:	train_loss: 1.053226:	test_loss: 0.967539:	
epoch: 235.000000:	train_loss: 1.047866:	test_loss: 0.972999:	
epoch: 236.000000:	train_loss: 1.048181:	test_loss: 0.969794:	
epoch: 237.000000:	train_loss: 1.049309:	test_loss: 0.970982:	
epoch: 238.000000:	train_loss: 1.052061:	test_loss: 0.972427:	
epoch: 239.000000:	train_loss: 1.047902:	test_loss: 0.975795:	
epoch: 240.000000:	train_loss: 1.043362:	test_loss: 0.981076:	
epoch: 241.000000:	train_loss: 1.047267:	test_loss: 1.012237:	
epoch: 242.000000:	train_loss: 1.041435:	test_loss: 0.963515:	
epoch: 243.000000:	train_loss: 1.042129:	test_loss: 0.972473:	
epoch: 244.000000:	train_loss: 1.044583:	test_loss: 0.957765:	
epoch: 245.000000:	train_loss: 1.042873:	test_loss: 0.967070:	
epoch: 246.000000:	train_loss: 1.042307:	test_loss: 0.955151:	
epoch: 247.000000:	train_loss: 1.038695:	test_loss: 0.969375:	
epoch: 248.000000:	train_loss: 1.042124:	test_loss: 0.957137:	
epoch: 249.000000:	train_loss: 1.046286:	test_loss: 0.987688:	
epoch: 250.000000:	train_loss: 1.036556:	test_loss: 0.968398:	
epoch: 251.000000:	train_loss: 1.042060:	test_loss: 0.962684:	
epoch: 252.000000:	train_loss: 1.036029:	test_loss: 0.956790:	
epoch: 253.000000:	train_loss: 1.037324:	test_loss: 0.972408:	
epoch: 254.000000:	train_loss: 1.039439:	test_loss: 0.967522:	
epoch: 255.000000:	train_loss: 1.031762:	test_loss: 0.959095:	
epoch: 256.000000:	train_loss: 1.035363:	test_loss: 0.980404:	
epoch: 257.000000:	train_loss: 1.031571:	test_loss: 0.972277:	
epoch: 258.000000:	train_loss: 1.038340:	test_loss: 0.962300:	
epoch: 259.000000:	train_loss: 1.037805:	test_loss: 0.968123:	
epoch: 260.000000:	train_loss: 1.036095:	test_loss: 0.971714:	
epoch: 261.000000:	train_loss: 1.030952:	test_loss: 0.964614:	
epoch: 262.000000:	train_loss: 1.031580:	test_loss: 0.962721:	
epoch: 263.000000:	train_loss: 1.035297:	test_loss: 0.962650:	
epoch: 264.000000:	train_loss: 1.032818:	test_loss: 0.950069:	
epoch: 265.000000:	train_loss: 1.031308:	test_loss: 0.953758:	
epoch: 266.000000:	train_loss: 1.034279:	test_loss: 0.953877:	
epoch: 267.000000:	train_loss: 1.026830:	test_loss: 0.951896:	
epoch: 268.000000:	train_loss: 1.030927:	test_loss: 0.962222:	
epoch: 269.000000:	train_loss: 1.039992:	test_loss: 0.951363:	
epoch: 270.000000:	train_loss: 1.028964:	test_loss: 0.950652:	
epoch: 271.000000:	train_loss: 1.027551:	test_loss: 0.957101:	
epoch: 272.000000:	train_loss: 1.025756:	test_loss: 0.966925:	
epoch: 273.000000:	train_loss: 1.030266:	test_loss: 0.958718:	
epoch: 274.000000:	train_loss: 1.023599:	test_loss: 0.985061:	
epoch: 275.000000:	train_loss: 1.022733:	test_loss: 0.959087:	
epoch: 276.000000:	train_loss: 1.023742:	test_loss: 0.954805:	
epoch: 277.000000:	train_loss: 1.027217:	test_loss: 0.960956:	
epoch: 278.000000:	train_loss: 1.031841:	test_loss: 0.949380:	
epoch: 279.000000:	train_loss: 1.028226:	test_loss: 0.941007:	
epoch: 280.000000:	train_loss: 1.025128:	test_loss: 0.968296:	
epoch: 281.000000:	train_loss: 1.028954:	test_loss: 0.972120:	
epoch: 282.000000:	train_loss: 1.018021:	test_loss: 0.965946:	
epoch: 283.000000:	train_loss: 1.025542:	test_loss: 0.962074:	
epoch: 284.000000:	train_loss: 1.026866:	test_loss: 0.940443:	
epoch: 285.000000:	train_loss: 1.029119:	test_loss: 0.955489:	
epoch: 286.000000:	train_loss: 1.020231:	test_loss: 0.968183:	
epoch: 287.000000:	train_loss: 1.025820:	test_loss: 0.954046:	
epoch: 288.000000:	train_loss: 1.030825:	test_loss: 0.964228:	
epoch: 289.000000:	train_loss: 1.022422:	test_loss: 0.960624:	
epoch: 290.000000:	train_loss: 1.020889:	test_loss: 0.953492:	
epoch: 291.000000:	train_loss: 1.018167:	test_loss: 0.954371:	
epoch: 292.000000:	train_loss: 1.020219:	test_loss: 0.950896:	
epoch: 293.000000:	train_loss: 1.020977:	test_loss: 0.956258:	
epoch: 294.000000:	train_loss: 1.021320:	test_loss: 0.949591:	
epoch: 295.000000:	train_loss: 1.015583:	test_loss: 0.948758:	
epoch: 296.000000:	train_loss: 1.023999:	test_loss: 0.962277:	
epoch: 297.000000:	train_loss: 1.018828:	test_loss: 0.960233:	
epoch: 298.000000:	train_loss: 1.024029:	test_loss: 0.959230:	
epoch: 299.000000:	train_loss: 1.018930:	test_loss: 0.935572:	
epoch: 300.000000:	train_loss: 1.024595:	test_loss: 0.952932:	
epoch: 301.000000:	train_loss: 1.017980:	test_loss: 0.949014:	
epoch: 302.000000:	train_loss: 1.017545:	test_loss: 0.944169:	
epoch: 303.000000:	train_loss: 1.015050:	test_loss: 0.946702:	
epoch: 304.000000:	train_loss: 1.015671:	test_loss: 0.947134:	
epoch: 305.000000:	train_loss: 1.020557:	test_loss: 0.944074:	
epoch: 306.000000:	train_loss: 1.013615:	test_loss: 0.950485:	
epoch: 307.000000:	train_loss: 1.015570:	test_loss: 0.946928:	
epoch: 308.000000:	train_loss: 1.015444:	test_loss: 0.951890:	
epoch: 309.000000:	train_loss: 1.013977:	test_loss: 0.963686:	
epoch: 310.000000:	train_loss: 1.012078:	test_loss: 0.939881:	
epoch: 311.000000:	train_loss: 1.014928:	test_loss: 0.938708:	
epoch: 312.000000:	train_loss: 1.015898:	test_loss: 0.939141:	
epoch: 313.000000:	train_loss: 1.022836:	test_loss: 0.949746:	
epoch: 314.000000:	train_loss: 1.020451:	test_loss: 0.940000:	
epoch: 315.000000:	train_loss: 1.015556:	test_loss: 0.953091:	
epoch: 316.000000:	train_loss: 1.017033:	test_loss: 0.946829:	
epoch: 317.000000:	train_loss: 1.012068:	test_loss: 0.937582:	
epoch: 318.000000:	train_loss: 1.016307:	test_loss: 0.942835:	
epoch: 319.000000:	train_loss: 1.010361:	test_loss: 0.941248:	
epoch: 320.000000:	train_loss: 1.015431:	test_loss: 0.941206:	
epoch: 321.000000:	train_loss: 1.008458:	test_loss: 0.956363:	
epoch: 322.000000:	train_loss: 1.017882:	test_loss: 0.935617:	
epoch: 323.000000:	train_loss: 1.010438:	test_loss: 0.941351:	
epoch: 324.000000:	train_loss: 1.016864:	test_loss: 0.958699:	
epoch: 325.000000:	train_loss: 1.016818:	test_loss: 0.966150:	
epoch: 326.000000:	train_loss: 1.015360:	test_loss: 0.968351:	
epoch: 327.000000:	train_loss: 1.011065:	test_loss: 0.933528:	
epoch: 328.000000:	train_loss: 1.011053:	test_loss: 0.940017:	
epoch: 329.000000:	train_loss: 1.019136:	test_loss: 0.939648:	
epoch: 330.000000:	train_loss: 1.014229:	test_loss: 0.944832:	
epoch: 331.000000:	train_loss: 1.012371:	test_loss: 0.944781:	
epoch: 332.000000:	train_loss: 1.017572:	test_loss: 0.948228:	
epoch: 333.000000:	train_loss: 1.012165:	test_loss: 0.957096:	
epoch: 334.000000:	train_loss: 1.009150:	test_loss: 0.948717:	
epoch: 335.000000:	train_loss: 1.007545:	test_loss: 0.942523:	
epoch: 336.000000:	train_loss: 1.012636:	test_loss: 0.956155:	
epoch: 337.000000:	train_loss: 1.015686:	test_loss: 0.959459:	
epoch: 338.000000:	train_loss: 1.011971:	test_loss: 0.942018:	
epoch: 339.000000:	train_loss: 1.013153:	test_loss: 0.941017:	
epoch: 340.000000:	train_loss: 1.011922:	test_loss: 0.941309:	
epoch: 341.000000:	train_loss: 1.011706:	test_loss: 0.969634:	
epoch: 342.000000:	train_loss: 1.012571:	test_loss: 0.957076:	
epoch: 343.000000:	train_loss: 1.012917:	test_loss: 0.957696:	
epoch: 344.000000:	train_loss: 1.014630:	test_loss: 0.946199:	
epoch: 345.000000:	train_loss: 1.012414:	test_loss: 0.949389:	
epoch: 346.000000:	train_loss: 1.006453:	test_loss: 0.940872:	
epoch: 347.000000:	train_loss: 1.007879:	test_loss: 0.936851:	
epoch: 348.000000:	train_loss: 1.005036:	test_loss: 0.955264:	
epoch: 349.000000:	train_loss: 1.010864:	test_loss: 0.926822:	
epoch: 350.000000:	train_loss: 1.011467:	test_loss: 0.937869:	
epoch: 351.000000:	train_loss: 1.010330:	test_loss: 0.939233:	
epoch: 352.000000:	train_loss: 1.007609:	test_loss: 0.932573:	
epoch: 353.000000:	train_loss: 1.004445:	test_loss: 0.937449:	
epoch: 354.000000:	train_loss: 1.003555:	test_loss: 0.937498:	
epoch: 355.000000:	train_loss: 1.007578:	test_loss: 0.935102:	
epoch: 356.000000:	train_loss: 1.009395:	test_loss: 0.943815:	
epoch: 357.000000:	train_loss: 1.005731:	test_loss: 0.945419:	
epoch: 358.000000:	train_loss: 1.011355:	test_loss: 0.950017:	
epoch: 359.000000:	train_loss: 1.010363:	test_loss: 0.947816:	
epoch: 360.000000:	train_loss: 1.009407:	test_loss: 0.932476:	
epoch: 361.000000:	train_loss: 1.007099:	test_loss: 0.941879:	
epoch: 362.000000:	train_loss: 1.004352:	test_loss: 0.926741:	
epoch: 363.000000:	train_loss: 1.003518:	test_loss: 0.932201:	
epoch: 364.000000:	train_loss: 1.009024:	test_loss: 0.941966:	
epoch: 365.000000:	train_loss: 1.010283:	test_loss: 0.930421:	
epoch: 366.000000:	train_loss: 1.006260:	test_loss: 0.951686:	
epoch: 367.000000:	train_loss: 1.008280:	test_loss: 0.931246:	
epoch: 368.000000:	train_loss: 1.004020:	test_loss: 0.931919:	
epoch: 369.000000:	train_loss: 1.009623:	test_loss: 0.930049:	
epoch: 370.000000:	train_loss: 1.010254:	test_loss: 0.933756:	
epoch: 371.000000:	train_loss: 1.005967:	test_loss: 0.925295:	
epoch: 372.000000:	train_loss: 1.007588:	test_loss: 0.930504:	
epoch: 373.000000:	train_loss: 1.006703:	test_loss: 0.936998:	
epoch: 374.000000:	train_loss: 1.003471:	test_loss: 0.934227:	
epoch: 375.000000:	train_loss: 1.004420:	test_loss: 0.944644:	
epoch: 376.000000:	train_loss: 1.004721:	test_loss: 0.926055:	
epoch: 377.000000:	train_loss: 1.003371:	test_loss: 0.929534:	
epoch: 378.000000:	train_loss: 1.009953:	test_loss: 0.923727:	
epoch: 379.000000:	train_loss: 1.006976:	test_loss: 0.936163:	
epoch: 380.000000:	train_loss: 1.011180:	test_loss: 0.951308:	
epoch: 381.000000:	train_loss: 1.004523:	test_loss: 0.929818:	
epoch: 382.000000:	train_loss: 1.009000:	test_loss: 0.932945:	
epoch: 383.000000:	train_loss: 1.002557:	test_loss: 0.942341:	
epoch: 384.000000:	train_loss: 1.007544:	test_loss: 0.928420:	
epoch: 385.000000:	train_loss: 1.003190:	test_loss: 0.937776:	
epoch: 386.000000:	train_loss: 1.006669:	test_loss: 0.941330:	
epoch: 387.000000:	train_loss: 1.011306:	test_loss: 0.944853:	
epoch: 388.000000:	train_loss: 1.005026:	test_loss: 0.932960:	
epoch: 389.000000:	train_loss: 1.001706:	test_loss: 0.934523:	
epoch: 390.000000:	train_loss: 1.010959:	test_loss: 0.961830:	
epoch: 391.000000:	train_loss: 0.998889:	test_loss: 0.945464:	
epoch: 392.000000:	train_loss: 1.003255:	test_loss: 0.980806:	
epoch: 393.000000:	train_loss: 1.002831:	test_loss: 0.940400:	
epoch: 394.000000:	train_loss: 1.002588:	test_loss: 0.928906:	
epoch: 395.000000:	train_loss: 1.003844:	test_loss: 0.941659:	
epoch: 396.000000:	train_loss: 1.002704:	test_loss: 0.930389:	
epoch: 397.000000:	train_loss: 1.005176:	test_loss: 0.948065:	
epoch: 398.000000:	train_loss: 1.004771:	test_loss: 0.926522:	
epoch: 399.000000:	train_loss: 1.000958:	test_loss: 0.933840:	
epoch: 400.000000:	train_loss: 1.004728:	test_loss: 0.934860:	
epoch: 401.000000:	train_loss: 1.002035:	test_loss: 0.929883:	
epoch: 402.000000:	train_loss: 1.006550:	test_loss: 0.929240:	
epoch: 403.000000:	train_loss: 1.008674:	test_loss: 0.954181:	
epoch: 404.000000:	train_loss: 1.000671:	test_loss: 0.930545:	
epoch: 405.000000:	train_loss: 1.004402:	test_loss: 0.933686:	
epoch: 406.000000:	train_loss: 1.003094:	test_loss: 0.945798:	
epoch: 407.000000:	train_loss: 1.007173:	test_loss: 0.931769:	
epoch: 408.000000:	train_loss: 1.003054:	test_loss: 0.939108:	
epoch: 409.000000:	train_loss: 1.001040:	test_loss: 0.928074:	
epoch: 410.000000:	train_loss: 1.004978:	test_loss: 0.931135:	
epoch: 411.000000:	train_loss: 1.006701:	test_loss: 0.933165:	
epoch: 412.000000:	train_loss: 1.004608:	test_loss: 0.942400:	
epoch: 413.000000:	train_loss: 0.999874:	test_loss: 0.928944:	
epoch: 414.000000:	train_loss: 1.000201:	test_loss: 0.942271:	
epoch: 415.000000:	train_loss: 1.004668:	test_loss: 0.980478:	
epoch: 416.000000:	train_loss: 1.004132:	test_loss: 0.926205:	
epoch: 417.000000:	train_loss: 0.999804:	test_loss: 0.944172:	
epoch: 418.000000:	train_loss: 1.006792:	test_loss: 0.938968:	
epoch: 419.000000:	train_loss: 1.006285:	test_loss: 0.942836:	
epoch: 420.000000:	train_loss: 1.001756:	test_loss: 0.927457:	
epoch: 421.000000:	train_loss: 0.996231:	test_loss: 0.931833:	
epoch: 422.000000:	train_loss: 1.001006:	test_loss: 0.927699:	
epoch: 423.000000:	train_loss: 1.005381:	test_loss: 0.940233:	
epoch: 424.000000:	train_loss: 1.006699:	test_loss: 0.931859:	
epoch: 425.000000:	train_loss: 1.005427:	test_loss: 0.938443:	
epoch: 426.000000:	train_loss: 0.997925:	test_loss: 0.930241:	
epoch: 427.000000:	train_loss: 1.007262:	test_loss: 0.936776:	
epoch: 428.000000:	train_loss: 0.996070:	test_loss: 0.939460:	
epoch: 429.000000:	train_loss: 1.002559:	test_loss: 0.935990:	
epoch: 430.000000:	train_loss: 0.999647:	test_loss: 0.941432:	
epoch: 431.000000:	train_loss: 1.000878:	test_loss: 0.950341:	
epoch: 432.000000:	train_loss: 0.999329:	test_loss: 0.939621:	
epoch: 433.000000:	train_loss: 1.003196:	test_loss: 0.957995:	
epoch: 434.000000:	train_loss: 1.000595:	test_loss: 0.936406:	
epoch: 435.000000:	train_loss: 1.000606:	test_loss: 0.934162:	
epoch: 436.000000:	train_loss: 1.003233:	test_loss: 0.955204:	
epoch: 437.000000:	train_loss: 0.999254:	test_loss: 0.947797:	
epoch: 438.000000:	train_loss: 1.001412:	test_loss: 0.944301:	
epoch: 439.000000:	train_loss: 0.998806:	test_loss: 0.931871:	
epoch: 440.000000:	train_loss: 1.006387:	test_loss: 0.926655:	
epoch: 441.000000:	train_loss: 1.001262:	test_loss: 0.941014:	
epoch: 442.000000:	train_loss: 1.000739:	test_loss: 0.923651:	
epoch: 443.000000:	train_loss: 0.995482:	test_loss: 0.946952:	
epoch: 444.000000:	train_loss: 1.003943:	test_loss: 0.927684:	
epoch: 445.000000:	train_loss: 0.999161:	test_loss: 0.945299:	
epoch: 446.000000:	train_loss: 1.003680:	test_loss: 0.939278:	
epoch: 447.000000:	train_loss: 1.004079:	test_loss: 0.943001:	
epoch: 448.000000:	train_loss: 0.999047:	test_loss: 0.944024:	
epoch: 449.000000:	train_loss: 0.999019:	test_loss: 0.951477:	
epoch: 450.000000:	train_loss: 1.001444:	test_loss: 0.941331:	
epoch: 451.000000:	train_loss: 1.000077:	test_loss: 0.935519:	
epoch: 452.000000:	train_loss: 0.999756:	test_loss: 0.970667:	
epoch: 453.000000:	train_loss: 0.999255:	test_loss: 0.935579:	
epoch: 454.000000:	train_loss: 1.001907:	test_loss: 0.930444:	
epoch: 455.000000:	train_loss: 1.002182:	test_loss: 0.931996:	
epoch: 456.000000:	train_loss: 1.000356:	test_loss: 0.924252:	
epoch: 457.000000:	train_loss: 0.996459:	test_loss: 0.953147:	
epoch: 458.000000:	train_loss: 1.005859:	test_loss: 0.949612:	
epoch: 459.000000:	train_loss: 1.003447:	test_loss: 0.924093:	
epoch: 460.000000:	train_loss: 1.002422:	test_loss: 0.958671:	
epoch: 461.000000:	train_loss: 0.998708:	test_loss: 0.926099:	
epoch: 462.000000:	train_loss: 1.004071:	test_loss: 0.950583:	
epoch: 463.000000:	train_loss: 1.000888:	test_loss: 0.926213:	
epoch: 464.000000:	train_loss: 0.997016:	test_loss: 0.927112:	
epoch: 465.000000:	train_loss: 1.001256:	test_loss: 0.935566:	
epoch: 466.000000:	train_loss: 0.999502:	test_loss: 0.941620:	
epoch: 467.000000:	train_loss: 1.001232:	test_loss: 0.925179:	
epoch: 468.000000:	train_loss: 0.999614:	test_loss: 0.928222:	
epoch: 469.000000:	train_loss: 1.004594:	test_loss: 0.922521:	
epoch: 470.000000:	train_loss: 1.005099:	test_loss: 0.961273:	
epoch: 471.000000:	train_loss: 0.999588:	test_loss: 0.957952:	
epoch: 472.000000:	train_loss: 0.999179:	test_loss: 0.929177:	
epoch: 473.000000:	train_loss: 1.000891:	test_loss: 0.941103:	
epoch: 474.000000:	train_loss: 1.004448:	test_loss: 0.935686:	
epoch: 475.000000:	train_loss: 0.997091:	test_loss: 0.932176:	
epoch: 476.000000:	train_loss: 1.005513:	test_loss: 0.943394:	
epoch: 477.000000:	train_loss: 0.997507:	test_loss: 1.003461:	
epoch: 478.000000:	train_loss: 1.001742:	test_loss: 1.000930:	
epoch: 479.000000:	train_loss: 1.001328:	test_loss: 0.943717:	
epoch: 480.000000:	train_loss: 1.002363:	test_loss: 0.926915:	
epoch: 481.000000:	train_loss: 1.004626:	test_loss: 0.933500:	
epoch: 482.000000:	train_loss: 1.000259:	test_loss: 0.928399:	
epoch: 483.000000:	train_loss: 1.000417:	test_loss: 0.930183:	
epoch: 484.000000:	train_loss: 1.004707:	test_loss: 0.930806:	
epoch: 485.000000:	train_loss: 1.002355:	test_loss: 0.936225:	
epoch: 486.000000:	train_loss: 0.999602:	test_loss: 0.930366:	
epoch: 487.000000:	train_loss: 1.001453:	test_loss: 0.928532:	
epoch: 488.000000:	train_loss: 1.003677:	test_loss: 0.930953:	
epoch: 489.000000:	train_loss: 0.999742:	test_loss: 0.926803:	
epoch: 490.000000:	train_loss: 1.002879:	test_loss: 0.934671:	
epoch: 491.000000:	train_loss: 1.006717:	test_loss: 0.974160:	
epoch: 492.000000:	train_loss: 0.997528:	test_loss: 0.930540:	
epoch: 493.000000:	train_loss: 1.001991:	test_loss: 0.942170:	
epoch: 494.000000:	train_loss: 0.999648:	test_loss: 0.930071:	
epoch: 495.000000:	train_loss: 1.002502:	test_loss: 0.929078:	
